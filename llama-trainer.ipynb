{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlamaModel(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        # Ensure return_dict is True to work with Trainer properly\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        # If labels are provided, calculate the loss\n",
    "        # if labels is not None:\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs.logits[:, :-1, :].contiguous().view(-1,\n",
    "                            self.config.vocab_size), labels[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            return CausalLMOutputWithPast(\n",
    "                loss=loss,\n",
    "                logits=outputs.logits,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "        # If no labels, just return the original model output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for a hypothetical 1B parameter model\n",
    "config_1B = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=4096,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    max_position_embeddings=context_length,\n",
    "    pad_token_id=2,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with bfloat16 precision\n",
    "model = CustomLlamaModel(config_1B)\n",
    "# model = model.half()\n",
    "model = model.to(device)  # Move model to GPU\n",
    "model = model.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to end-of-sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset (example using 'wikimedia/wikipedia', '20231101.en' subset)\n",
    "dataset = load_dataset(\"D:/ai-stuff/datasets/wikipedia\", \"20231101.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_dataset = dataset[\"train\"].select(range(2))\n",
    "exam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_batches = tokenizer(\n",
    "    exam_dataset[\"text\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=int(round(context_length / 4)),\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tokenized_batches:\n",
    "    print(f\"{key}: {tokenized_batches[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display the new tokenized text\n",
    "decoded_text = []\n",
    "for i in range(len(tokenized_batches[\"input_ids\"])):\n",
    "    decoded_text.append(tokenizer.decode(tokenized_batches[\"input_ids\"][i]))\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset[\"train\"].select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize all texts and return overflow tokens as separate examples\n",
    "    tokenized_batches = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",  # or False if you prefer dynamic padding later on\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=int(round(context_length / 4)),\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Initialize lists to collect new examples\n",
    "    overflow_input_ids = []\n",
    "    overflow_labels = []\n",
    "\n",
    "    for batch_index in range(len(tokenized_batches[\"input_ids\"])):\n",
    "        # If there are overflow tokens, they would be in 'overflowing_tokens' under 'encodings'\n",
    "        num_overflows = len(\n",
    "            tokenized_batches.encodings[batch_index].overflowing) if \"overflowing_tokens\" in tokenized_batches else 0\n",
    "\n",
    "        # For the main input_ids batch and any overflow batches\n",
    "        for overflow_index in range(num_overflows + 1):  # +1 to include the main batch itself\n",
    "            if overflow_index == 0:  # The main batch\n",
    "                input_ids = tokenized_batches[\"input_ids\"][batch_index].tolist()\n",
    "            else:  # Overflow batches\n",
    "                input_ids = tokenized_batches.encodings[batch_index].overflowing[overflow_index - 1].ids\n",
    "\n",
    "            labels = input_ids[1:] + [-100]  # Create labels shifted by one position\n",
    "            overflow_input_ids.append(input_ids)\n",
    "            overflow_labels.append(labels)\n",
    "\n",
    "    # Prepare the final output as a dictionary\n",
    "    output = {\"input_ids\": overflow_input_ids, \"labels\": overflow_labels}\n",
    "\n",
    "    # Convert to Dataset format\n",
    "    return datasets.Dataset.from_dict(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train_dataset = tokenize_function(small_train_dataset)\n",
    "tokenised_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_lengths = [len(ids) for ids in tokenised_train_dataset['input_ids']]\n",
    "min_length = min(input_ids_lengths)\n",
    "max_length = max(input_ids_lengths)\n",
    "\n",
    "print(\"Minimum length:\", min_length)\n",
    "print(\"Maximum length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True)\n",
    "tokenized_eval = dataset[\"test\"].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # warmup_steps=5,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"loss\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    bf16=True,  # Enable mixed-precision training\n",
    "    bf16_full_eval=True,  # Enable mixed-precision evaluation\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW optimizer\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(\"./custom_llama_1B_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bff5e735b6349eb955c1c10a59429ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d1c87be8194b8d98b5405e743288cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from CustomLlamaModel import CustomLlamaModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import LlamaConfig, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# Model settings\n",
    "hidden_layers = 8  # Number of transformer layers\n",
    "hidden_size = 1024  # Size of the hidden states in the transformer layers\n",
    "intermediate_size = 2048  # Size of the feed-forward network in the transformer layers\n",
    "attention_heads = 32  # Number of attention heads\n",
    "context_length = 2048  # Length of the input context\n",
    "stride = 50  # Stride for splitting the input into multiple sequences\n",
    "\n",
    "# Training settings\n",
    "seed = 42\n",
    "epochs = 20  # Number of training epochs\n",
    "batch_size = 2  # Number of sequences to process in parallel\n",
    "gradient_accumulation_steps = 10  # Number of update steps to accumulate before performing a backward pass\n",
    "logging_steps = 1  # Log training loss every X steps\n",
    "warmup_steps = 100 / gradient_accumulation_steps  # Number of warmup steps for the learning rate scheduler\n",
    "\n",
    "run = \"2\"\n",
    "output_dir = \"./results/run-\" + run\n",
    "logging_dir = output_dir + \"/logs\"\n",
    "final_dir = \"./final\"\n",
    "\n",
    "learning_rate = 5e-5\n",
    "lr_scheduler_type = \"linear\"\n",
    "optim = \"adamw_torch\"  # Use PyTorch's AdamW optimizer\n",
    "\n",
    "evaluation_strategy = \"epoch\"\n",
    "eval_steps = 0.25\n",
    "save_strategy = \"epoch\"\n",
    "save_steps = 0.25\n",
    "\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model = \"loss\"\n",
    "\n",
    "# Write the configuration to a JSON file\n",
    "training_config = {\n",
    "    \"hidden_layers\": hidden_layers,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"intermediate_size\": intermediate_size,\n",
    "    \"attention_heads\": attention_heads,\n",
    "    \"context_length\": context_length,\n",
    "    \"stride\": stride,\n",
    "    \"seed\": seed,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \"logging_steps\": logging_steps,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"lr_scheduler_type\": lr_scheduler_type,\n",
    "    \"optim\": optim,\n",
    "    \"evaluation_strategy\": evaluation_strategy,\n",
    "    \"eval_steps\": eval_steps,\n",
    "    \"save_strategy\": save_strategy,\n",
    "    \"save_steps\": save_steps,\n",
    "    \"load_best_model_at_end\": load_best_model_at_end,\n",
    "    \"metric_for_best_model\": metric_for_best_model,\n",
    "    \"start_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(output_dir + \"/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=4)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration for a hypothetical 1B parameter model\n",
    "config_1B = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_hidden_layers=hidden_layers,\n",
    "    num_attention_heads=attention_heads,\n",
    "    max_position_embeddings=context_length,\n",
    "    pad_token_id=2,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Initialize the model with bfloat16 precision\n",
    "model = CustomLlamaModel(config_1B)\n",
    "# model = model.half()  # Convert model parameters to bfloat16\n",
    "model = model.to(device)  # Move model to GPU\n",
    "model = model.train()  # Set model to training mode\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to end-of-sequence token\n",
    "\n",
    "# Prepare dataset (example using 'wikimedia/wikipedia', '20231101.en' subset)\n",
    "dataset = load_dataset(\"D:/ai-stuff/datasets/wikipedia\", \"20231101.en\")\n",
    "\n",
    "# Select the first 10000 examples\n",
    "dataset = dataset[\"train\"].select(range(10000))\n",
    "\n",
    "# Merge the \"title\" and \"text\" columns into a single \"text\" column, separated by two newline characters\n",
    "# dataset = dataset.map(lambda examples: {\n",
    "#     \"text\": examples[\"title\"] + \"\\n\\n\" + examples[\"text\"]\n",
    "# })\n",
    "\n",
    "# Shuffling the dataset\n",
    "dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "# Split the dataset into training and evaluation sets 90:10\n",
    "dataset = dataset.train_test_split(test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize all texts and return overflow tokens as separate examples\n",
    "    tokenized_batches = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=stride,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Shift the input ids to the left to create the labels so that the model predicts the next token.\n",
    "    # The label for the last token is set to -100, so it's ignored by the loss function.\n",
    "    tokenized_batches[\"labels\"] = tokenized_batches.input_ids.clone()\n",
    "    tokenized_batches[\"labels\"][:, :-1] = tokenized_batches[\"labels\"][:, 1:].clone()\n",
    "    tokenized_batches[\"labels\"][:, -1] = -100\n",
    "\n",
    "    return tokenized_batches\n",
    "\n",
    "\n",
    "def tokenize_function_2(examples):\n",
    "    # Tokenize all texts and truncate them to the maximum length\n",
    "    tokenized_batches = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Shift the input ids to the left to create the labels so that the model predicts the next token.\n",
    "    # The label for the last token is set to -100, so it's ignored by the loss function.\n",
    "    tokenized_batches[\"labels\"] = tokenized_batches.input_ids.clone()\n",
    "    tokenized_batches[\"labels\"][:, :-1] = tokenized_batches[\"labels\"][:, 1:].clone()\n",
    "    tokenized_batches[\"labels\"][:, -1] = -100\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenize_function(dataset[\"train\"])\n",
    "tokenized_eval = tokenize_function(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052aff509f8a4985a5cb8bd5c72e4634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be785308e614e6b9e7d1f9933bf1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokens = dataset[\"train\"].map(tokenize_function_2, batched=True)\n",
    "test_tokens = dataset[\"test\"].map(tokenize_function_2, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 9000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12009,   433,  ...,     2,     2,     2],\n",
       "        [    1,   402,  1646,  ...,     2,     2,     2],\n",
       "        [    1,   450,   306,  ...,  5925, 13457,   297],\n",
       "        ...,\n",
       "        [    1,   315,   804,  ...,   728,  1056, 28572],\n",
       "        [    1, 13215, 15864,  ..., 29945,   259,   785],\n",
       "        [    1, 29892,  3082,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'overflow_to_sample_mapping': tensor([   0,    1,    2,  ..., 8999, 8999, 8999]), 'labels': tensor([[12009,   433,  3851,  ...,     2,     2,  -100],\n",
       "        [  402,  1646,  6626,  ...,     2,     2,  -100],\n",
       "        [  450,   306, 29956,  ..., 13457,   297,  -100],\n",
       "        ...,\n",
       "        [  315,   804,   279,  ...,  1056, 28572,  -100],\n",
       "        [13215, 15864, 13158,  ...,   259,   785,  -100],\n",
       "        [29892,  3082, 17739,  ...,     2,     2,  -100]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = Dataset.from_dict(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'overflow_to_sample_mapping', 'labels'],\n",
       "    num_rows: 16535\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
