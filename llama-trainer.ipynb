{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U jupyter ipython ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaConfig, AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"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\"\n",
    "\n",
    "context_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlamaModel(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        # Ensure return_dict is True to work with Trainer properly\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=True,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        # If labels are provided, calculate the loss\n",
    "        # if labels is not None:\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(outputs.logits[:, :-1, :].contiguous().view(-1,\n",
    "                            self.config.vocab_size), labels[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            return CausalLMOutputWithPast(\n",
    "                loss=loss,\n",
    "                logits=outputs.logits,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "        # If no labels, just return the original model output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 7B config\n",
    "config_1B = LlamaConfig.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_token)\n",
    "config_1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for a hypothetical 1B parameter model\n",
    "config_1B.hidden_size = 1024\n",
    "config_1B.intermediate_size = 4096\n",
    "config_1B.num_hidden_layers = 24\n",
    "config_1B.num_attention_heads = 16\n",
    "config_1B.max_position_embeddings = context_length\n",
    "config_1B.pad_token_id = config_1B.eos_token_id\n",
    "config_1B.torch_dtype = \"bfloat16\"\n",
    "config_1B.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "config_1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LlamaForCausalLM(config_1B)\n",
    "model = model.to(device)  # Move model to GPU\n",
    "model = model.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_token)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to end-of-sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset (example using 'wikimedia/wikipedia', '20231101.en' subset)\n",
    "# dataset = load_dataset(\"D:/ai-stuff/datasets/wikipedia\", \"20231101.en\")\n",
    "dataset = load_dataset(\"/media/gronkomatic/Embiggen/ai-stuff/datasets/wikipedia\", \"20231101.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_dataset = dataset[\"train\"].select(range(2))\n",
    "exam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_batches = tokenizer(\n",
    "    exam_dataset[\"text\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=int(round(context_length / 4)),\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tokenized_batches:\n",
    "    print(f\"{key}: {tokenized_batches[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display the new tokenized text\n",
    "decoded_text = []\n",
    "for i in range(len(tokenized_batches[\"input_ids\"])):\n",
    "    decoded_text.append(tokenizer.decode(tokenized_batches[\"input_ids\"][i]))\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = dataset[\"train\"].select(range(1000))\n",
    "small_eval_dataset = dataset[\"test\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",  # or False if you prefer dynamic padding later on\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=int(round(context_length / 4)),\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "# tokenized_train = dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "# tokenized_eval = dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "tokenized_train = small_train_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_eval = small_eval_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"loss\",\n",
    "    gradient_accumulation_steps=8,\n",
    "    bf16=True,  # Enable mixed-precision training\n",
    "    bf16_full_eval=True,  # Enable mixed-precision evaluation\n",
    "    optim=\"adamw_torch\",  # Use PyTorch's AdamW optimizer\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_pretrained(\"./custom_llama_1B_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from CustomLlamaModel import CustomLlamaModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import LlamaConfig, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# Model settings\n",
    "hidden_layers = 8  # Number of transformer layers\n",
    "hidden_size = 1024  # Size of the hidden states in the transformer layers\n",
    "intermediate_size = 2048  # Size of the feed-forward network in the transformer layers\n",
    "attention_heads = 32  # Number of attention heads\n",
    "context_length = 2048  # Length of the input context\n",
    "stride = 50  # Stride for splitting the input into multiple sequences\n",
    "\n",
    "# Training settings\n",
    "seed = 42\n",
    "epochs = 20  # Number of training epochs\n",
    "batch_size = 2  # Number of sequences to process in parallel\n",
    "gradient_accumulation_steps = 10  # Number of update steps to accumulate before performing a backward pass\n",
    "logging_steps = 1  # Log training loss every X steps\n",
    "warmup_steps = 100 / gradient_accumulation_steps  # Number of warmup steps for the learning rate scheduler\n",
    "\n",
    "run = \"2\"\n",
    "output_dir = \"./results/run-\" + run\n",
    "logging_dir = output_dir + \"/logs\"\n",
    "final_dir = \"./final\"\n",
    "\n",
    "learning_rate = 5e-5\n",
    "lr_scheduler_type = \"linear\"\n",
    "optim = \"adamw_torch\"  # Use PyTorch's AdamW optimizer\n",
    "\n",
    "evaluation_strategy = \"epoch\"\n",
    "eval_steps = 0.25\n",
    "save_strategy = \"epoch\"\n",
    "save_steps = 0.25\n",
    "\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model = \"loss\"\n",
    "\n",
    "# Write the configuration to a JSON file\n",
    "training_config = {\n",
    "    \"hidden_layers\": hidden_layers,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"intermediate_size\": intermediate_size,\n",
    "    \"attention_heads\": attention_heads,\n",
    "    \"context_length\": context_length,\n",
    "    \"stride\": stride,\n",
    "    \"seed\": seed,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "    \"logging_steps\": logging_steps,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"lr_scheduler_type\": lr_scheduler_type,\n",
    "    \"optim\": optim,\n",
    "    \"evaluation_strategy\": evaluation_strategy,\n",
    "    \"eval_steps\": eval_steps,\n",
    "    \"save_strategy\": save_strategy,\n",
    "    \"save_steps\": save_steps,\n",
    "    \"load_best_model_at_end\": load_best_model_at_end,\n",
    "    \"metric_for_best_model\": metric_for_best_model,\n",
    "    \"start_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "}\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "with open(output_dir + \"/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=4)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration for a hypothetical 1B parameter model\n",
    "config_1B = LlamaConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_hidden_layers=hidden_layers,\n",
    "    num_attention_heads=attention_heads,\n",
    "    max_position_embeddings=context_length,\n",
    "    pad_token_id=2,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "# Initialize the model with bfloat16 precision\n",
    "model = CustomLlamaModel(config_1B)\n",
    "# model = model.half()  # Convert model parameters to bfloat16\n",
    "model = model.to(device)  # Move model to GPU\n",
    "model = model.train()  # Set model to training mode\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to end-of-sequence token\n",
    "\n",
    "# Prepare dataset (example using 'wikimedia/wikipedia', '20231101.en' subset)\n",
    "dataset = load_dataset(\"D:/ai-stuff/datasets/wikipedia\", \"20231101.en\")\n",
    "\n",
    "# Select the first 10000 examples\n",
    "dataset = dataset[\"train\"].select(range(10000))\n",
    "\n",
    "# Merge the \"title\" and \"text\" columns into a single \"text\" column, separated by two newline characters\n",
    "# dataset = dataset.map(lambda examples: {\n",
    "#     \"text\": examples[\"title\"] + \"\\n\\n\" + examples[\"text\"]\n",
    "# })\n",
    "\n",
    "# Shuffling the dataset\n",
    "dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "# Split the dataset into training and evaluation sets 90:10\n",
    "dataset = dataset.train_test_split(test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize all texts and return overflow tokens as separate examples\n",
    "    tokenized_batches = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=stride,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Shift the input ids to the left to create the labels so that the model predicts the next token.\n",
    "    # The label for the last token is set to -100, so it's ignored by the loss function.\n",
    "    tokenized_batches[\"labels\"] = tokenized_batches.input_ids.clone()\n",
    "    tokenized_batches[\"labels\"][:, :-1] = tokenized_batches[\"labels\"][:, 1:].clone()\n",
    "    tokenized_batches[\"labels\"][:, -1] = -100\n",
    "\n",
    "    return tokenized_batches\n",
    "\n",
    "\n",
    "def tokenize_function_2(examples):\n",
    "    # Tokenize all texts and truncate them to the maximum length\n",
    "    tokenized_batches = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Shift the input ids to the left to create the labels so that the model predicts the next token.\n",
    "    # The label for the last token is set to -100, so it's ignored by the loss function.\n",
    "    tokenized_batches[\"labels\"] = tokenized_batches.input_ids.clone()\n",
    "    tokenized_batches[\"labels\"][:, :-1] = tokenized_batches[\"labels\"][:, 1:].clone()\n",
    "    tokenized_batches[\"labels\"][:, -1] = -100\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenize_function(dataset[\"train\"])\n",
    "tokenized_eval = tokenize_function(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = dataset[\"train\"].map(tokenize_function_2, batched=True)\n",
    "test_tokens = dataset[\"test\"].map(tokenize_function_2, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = Dataset.from_dict(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
