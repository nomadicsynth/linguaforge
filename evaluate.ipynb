{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-2929',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-5858',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-8787',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-11716',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-14645',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-17574',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-20503',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-23432',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-26361',\n",
       " '/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/checkpoint-29288']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a natural-sorted list of the checkpoint paths\n",
    "model_path = \"/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/training-run-20240630-213618/\"\n",
    "checkpoints = sorted(\n",
    "    [\n",
    "        os.path.join(model_path, f)\n",
    "        for f in os.listdir(model_path)\n",
    "        if f.startswith(\"checkpoint\")\n",
    "    ],\n",
    "    key=lambda x: int(x.split(\"-\")[-1]),\n",
    ")\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text', 'idx', 'topic', 'model_name', 'hash', 'language', 'custom_instruction', 'source', 'system_prompt', 'category', 'skip_prompt_formatting', 'avatarUrl', 'views', 'model'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the eval dataset\n",
    "dataset_name = \"/mnt/ai-stuff-fast/training-results/mini-mistral-wikipedia-20231101.en-science-sci-fi-OpenHermes-2.5-chatML-Grokfast/dataset\"\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(dataset_name)\n",
    "dataset = dataset[\"test\"].shuffle(seed=42).select(range(10))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints[0])\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics\n",
    "# accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "# precision = evaluate.load(\"precision\")\n",
    "# recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a test, load the first checkpoint and evaluate it\n",
    "checkpoint = checkpoints[0]\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
    "model.to(\"cuda:1\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"batch_size\": 1,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=checkpoints[0],\n",
    "    device=torch.device(\"cuda:1\"),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    **generation_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m task_evaluator \u001b[38;5;241m=\u001b[39m evaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtask_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ai-stuff-fast/training-results/grokking/transformers-training/.venv/lib/python3.12/site-packages/evaluate/evaluator/base.py:261\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    258\u001b[0m metric_inputs\u001b[38;5;241m.\u001b[39mupdate(predictions)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Compute metrics from references and predictions\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m metric_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_resamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# TODO: To clarify why `wer` and `cer` return float\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# even though metric.compute contract says that it\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# returns Optional[dict].\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(metric_results) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m:\n",
      "File \u001b[0;32m/mnt/ai-stuff-fast/training-results/grokking/transformers-training/.venv/lib/python3.12/site-packages/evaluate/evaluator/base.py:527\u001b[0m, in \u001b[0;36mEvaluator.compute_metric\u001b[0;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metric\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     metric: EvaluationModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     random_state: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ):\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute and return metrics.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMETRIC_KWARGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    530\u001b[0m         metric_keys \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m/mnt/ai-stuff-fast/training-results/grokking/transformers-training/.venv/lib/python3.12/site-packages/evaluate/module.py:456\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ai-stuff-fast/training-results/grokking/transformers-training/.venv/lib/python3.12/site-packages/evaluate/module.py:399\u001b[0m, in \u001b[0;36mEvaluationModule._finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Let's acquire a lock on each node files to be sure they are finished writing\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     file_paths, filelocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_cache_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Read the predictions and references\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ai-stuff-fast/training-results/grokking/transformers-training/.venv/lib/python3.12/site-packages/evaluate/module.py:316\u001b[0m, in \u001b[0;36mEvaluationModule._get_all_cache_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_process \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    317\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation module cache file doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist. Please make sure that you call `add` or `add_batch` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least once before calling `compute`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`."
     ]
    }
   ],
   "source": [
    "# Create the evaluator\n",
    "task_evaluator = evaluator(\"text-generation\")\n",
    "\n",
    "# Evaluate the model\n",
    "results = task_evaluator.compute(model_or_pipeline=pipe, data=dataset, metric=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=True\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for batch in dataset:\n",
    "        inputs = {\n",
    "            key: batch[key].to(model.device) for key in tokenizer.model_input_names\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        references.extend(batch[\"label\"])\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=references),\n",
    "        \"f1\": f1.compute(predictions=predictions, references=references),\n",
    "        \"precision\": precision.compute(predictions=predictions, references=references),\n",
    "        \"recall\": recall.compute(predictions=predictions, references=references),\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare the results dataframe\n",
    "results = []\n",
    "\n",
    "# Iterate through checkpoints and evaluate\n",
    "for checkpoint_dir in sorted(os.listdir(checkpoints_path)):\n",
    "    checkpoint_path = os.path.join(checkpoints_path, checkpoint_dir)\n",
    "    if os.path.isdir(checkpoint_path):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "        metrics = evaluate_model(model, encoded_dataset)\n",
    "        metrics[\"checkpoint\"] = checkpoint_dir\n",
    "        results.append(metrics)\n",
    "\n",
    "# Save results to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "\n",
    "print(\"Evaluation completed and results saved to evaluation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
