{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"template_model_name\"] = \"mistralai/Mistral-7B-v0.1\"\n",
    "args[\"sequence_length\"] = 2048\n",
    "args[\"stride\"] = 64\n",
    "args[\"additional_special_tokens\"] = []\n",
    "args[\"chat_template\"] = None\n",
    "args[\"dataset_path\"] = \"wikitext\"\n",
    "args[\"dataset_size\"] = 0\n",
    "args[\"dataset_split\"] = 0.9\n",
    "args[\"seed\"] = 42\n",
    "args[\"shuffle\"] = False\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading the tokenizer from {args['template_model_name']}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"template_model_name\"])\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer.model_max_length = args[\"sequence_length\"]\n",
    "\n",
    "# Add assistant token to the tokenizer for the chat template because it is not in the vocabulary without a space in front of it!\n",
    "tokenizer.add_tokens([\"assistant\"])\n",
    "print(\n",
    "    f\"'assistant' token added to the tokenizer because it is not in the vocabulary without a space in front of it!\"\n",
    ")\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "# Add \"<|im_start|>\", \"<|im_end|>\", \"<|pause|>\", \"<|mem_start|>\", \"<|mem_end|>\", etc.\n",
    "additional_special_tokens = [\n",
    "    \"<|im_start|>\",\n",
    "    \"<|im_end|>\",\n",
    "    \"<|named_user|>\",  # Named user. For future use. Example: \"<|im_start|><|named_user|>Alice\\n<Alice's message><|im_end|>\"\n",
    "    \"<|named_assistant|>\",  # Named assistant. For future use. Example: \"<|im_start|><|named_assistant|>Assistant George\\n<Assistant George's message><|im_end|>\"\n",
    "    \"<|mem_start|>\",\n",
    "    \"<|mem_end|>\",  # Memory start and end tokens. For future use. Store hidden information in the context, e.g. \"<|mem_start|>Alice's birthday is 12th May.<|mem_end|>\"\n",
    "    \"<|pause|>\",  # Pause token. For future use. See https://arxiv.org/abs/2310.02226.pdf Think before you speak: Training Language Models With Pause Tokens\n",
    "]\n",
    "\n",
    "# Add additional special tokens\n",
    "if args[\"additional_special_tokens\"]:\n",
    "    additional_special_tokens += args[\"additional_special_tokens\"]\n",
    "\n",
    "# Add <|spare_1|>, <|spare_2|>, etc. to the tokenizer to make the vocab size a multiple of 8\n",
    "for i in range(1, 8 - (len(tokenizer) + len(additional_special_tokens)) % 8 + 1):\n",
    "    additional_special_tokens.append(f\"<|spare_{i}|>\")\n",
    "\n",
    "if len(additional_special_tokens) > 0:\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": additional_special_tokens},\n",
    "        replace_additional_special_tokens=False,\n",
    "    )\n",
    "\n",
    "if args[\"additional_special_tokens\"]:\n",
    "    print(f\"Additional special tokens added to the tokenizer.\")\n",
    "\n",
    "    # Print the token IDs of the special tokens\n",
    "    for token in args[\"additional_special_tokens\"]:\n",
    "        print(f\"{token}: {tokenizer(token)}\")\n",
    "\n",
    "# Assert that the vocab size is a multiple of 8\n",
    "assert (\n",
    "    len(tokenizer)\n",
    ") % 8 == 0, \"The vocabulary size is not a multiple of 8. Fix the padding code, dumbass!\"\n",
    "\n",
    "# Set up the chat template\n",
    "if args[\"chat_template\"]:\n",
    "    tokenizer.chat_template = args[\"chat_template\"]\n",
    "\n",
    "print(f\"Tokeniser loaded with {len(tokenizer)} tokens in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "def prepare_dataset(\n",
    "    dataset: DatasetDict,\n",
    "    dataset_size: int,\n",
    "    dataset_split: float,\n",
    "    shuffle: bool = False,\n",
    "    seed: int = 42,\n",
    ") -> DatasetDict:\n",
    "    print(\"Preparing the dataset...\")\n",
    "    prepared_dataset = None\n",
    "\n",
    "    # Shuffle if required\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    # Select the first dataset_size examples from the training set\n",
    "    if dataset_size > 0:\n",
    "        print(\"Selecting\", dataset_size, \"examples from the dataset...\")\n",
    "        prepared_dataset = dataset[\"train\"].select(range(dataset_size))\n",
    "    else:\n",
    "        dataset_size = len(dataset[\"train\"])\n",
    "        print(\"Using the entire dataset of size\", dataset_size)\n",
    "        prepared_dataset = dataset[\"train\"]\n",
    "\n",
    "    # Split the dataset into training and evaluation sets (dataset_split% for training, 1-dataset_split% for evaluation)\n",
    "    print(\"Splitting the dataset into training and evaluation sets...\")\n",
    "    print(\"Training set size:\", round(dataset_size * dataset_split))\n",
    "    print(\"Evaluation set size:\", dataset_size - round(dataset_size * dataset_split))\n",
    "    prepared_dataset = prepared_dataset.train_test_split(\n",
    "        test_size=1 - dataset_split, seed=seed, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    # Return the training and evaluation datasets\n",
    "    return prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(f\"Loading the dataset from {args['dataset_path']}\")\n",
    "dataset = load_dataset(args[\"dataset_path\"])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "prepared_dataset = prepare_dataset(\n",
    "    dataset=dataset,\n",
    "    dataset_size=args[\"dataset_size\"],\n",
    "    dataset_split=args[\"dataset_split\"],\n",
    "    shuffle=args[\"shuffle\"],\n",
    "    seed=args[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a batch of texts\n",
    "total_tokens = 0\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    print(result[\"input_ids\"])\n",
    "    exit()\n",
    "    global total_tokens\n",
    "    total_tokens += len(result[\"input_ids\"])\n",
    "    return result\n",
    "\n",
    "# Tokenize the dataset\n",
    "total_tokens = 0\n",
    "\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized_dataset = prepared_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "total_tokens = 0\n",
    "\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized_dataset = prepared_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "print(\"Total tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tokens in the dataset. Use a tqdm progress bar to show progress.\n",
    "total_tokens = 0\n",
    "for example in tqdm(tokenized_dataset[\"train\"]):\n",
    "    total_tokens += len(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tokens in the dataset. Use a tqdm progress bar to show progress.\n",
    "total_tokens = sum(\n",
    "    len(example[\"input_ids\"]) for example in tqdm(tokenized_dataset[\"train\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of tokens in the dataset in a human-readable format\n",
    "print(\n",
    "    f\"Total number of tokens in the dataset: {total_tokens:,} ({total_tokens/1e9:.2f} billion tokens)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
